\begin{table*}[t]
    \centering
    \caption{\small{Comparison with low-rank algorithms on pre-training various sizes of LLaMA models on C4 dataset. Validation perplexity is reported, along with a memory estimate of the total of parameters and optimizer states based on BF16 format. The actual memory footprint of \lowrank{} is reported in Fig.~\ref{fig:memory_vs_model_size}.}}
    \label{tab:lora_compare_llama}
    \begin{tabular}{lcccc}
    \toprule
               & \textbf{60M} & \textbf{130M} & \textbf{350M} & \textbf{1B} \\
    \midrule
    Full-Rank & 34.06 (0.36G) & 25.08 (0.76G) & 18.80 (2.06G) & 15.56 (7.80G) \\
    \midrule
    \textbf{\lowrank} & \textbf{34.88} (0.24G) & \textbf{25.36} (0.52G) & \textbf{18.95} (1.22G) & \textbf{15.64} (4.38G) \\
    Low-Rank & 78.18 (0.26G) & 45.51 (0.54G) & 37.41 (1.08G) & 142.53 (3.57G) \\
    LoRA & 34.99 (0.36G) & 33.92 (0.80G) & 25.58 (1.76G) & 19.21 (6.17G) \\
    ReLoRA & 37.04 (0.36G) & 29.37 (0.80G) & 29.08 (1.76G) & 18.33 (6.17G) \\
    \bottomrule
    $r / d_{model}$ & 128 / 256 & 256 / 768 & 256 / 1024 & 512 / 2048 \\
    Training Tokens & 1.1B & 2.2B & 6.4B & 13.1B \\ %
    \bottomrule
    \end{tabular}
\end{table*}







