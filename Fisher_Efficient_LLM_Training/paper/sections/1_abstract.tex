\begin{abstract}
    Training large language models (LLMs) presents significant memory challenges due to the growing size of data, weights, and optimizer states. While techniques such as data and model parallelism, gradient checkpointing, and offloading strategies address this issue, they are often infeasible due to hardware constraints. Alternative methods, like Parameter Efficient Fine-Tuning (PEFT) and GaLore, mitigate memory usage by approximating weights or optimizer states. PEFT methods, such as LoRA and ReLoRa, have gained popularity for fine-tuning LLMs, though they require a full-rank warm start. In contrast, GaLore allows full-parameter learning while being more memory-efficient.
    In this work, we introduce \textbf{\lowrank}, which efficiently applies the inverse Empirical Fisher Information Matrix to low-rank gradients using the Woodbury Identity. We show that especially in the case of a limited iteration budget, incorporating second-order information can significantly improve the convergence rate. Empirical pre-training on 60M, 300M, and 1.1B parameter Llama models on C4 data demonstrates significantly lower perplexity over GaLore, with no memory overhead. Furthermore, fine-tuning the TinyLlama 1.1B model for function calling using the TinyAgent framework shows that \textbf{\lowrank} significantly outperforms LoRA, achieving 83.5\% accuracy and surpasses ChatGPT4o with 79.08\% on the TinyAgent dataset, all while using less memory.
\end{abstract}

