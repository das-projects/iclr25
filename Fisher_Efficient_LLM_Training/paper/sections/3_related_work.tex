
\section{Related Works}
\paragraph{Low-rank adaptation.}
\citet{huLoRALowRankAdaptation2021} proposed Low-Rank Adaptation (LoRA) to fine-tune pre-trained models with low-rank adaptors. 
This method reduces the memory footprint by maintaining a low-rank weight adaptor for each layer.
There are a few variants of LoRA proposed to enhance its performance \citep{renduchintalaTiedLoraEnhacingParameter2023, shengSLoRAServingThousands2023, zhangLORAFAMEMORYEFFICIENTLOWRANK, xiaChainLoRAEfficient2024}, supporting multi-task learning \citep{wangMultiLoRADemocratizingLoRA2023}, and further reducing the memory footprint \citep{dettmersQLoRAEfficientFinetuning2023}.
\citet{lialinReLoRAHighRankTraining2023} proposed ReLoRA, a variant of LoRA designed for pre-training, but requires a full-rank training warmup to achieve comparable performance as the standard baseline.
Inspired by LoRA, \citet{haoFloraLowRankAdapters2024} also suggested that gradients can be compressed in a low-rank subspace, and they proposed to use random projections to compress the gradients.
There have also been approaches that propose training networks with low-rank factorized weights from scratch \citep{kamalakaraExploringLowRank2022,wangCuttlefishLowrankModel2023,zhaoInRankIncrementalLowRank2023}.


\paragraph{Subspace learning.}
Recent studies have demonstrated that the learning primarily occurs within a significantly low-dimensional parameter subspace \citep{gur-ariGradientDescentHappens2018,larsenHowManyDegrees2022}.
These findings promote a special type of learning called \textit{subspace learning}, where the model weights are optimized within a low-rank subspace. 
This notion has been widely used in different domains of machine learning, including meta-learning and continual learning \citep{leeGradientBasedMetaLearningLearned2018,chaudhryContinualLearningLowrank2020}.

\paragraph{Projected gradient descent.}
\lowrank{} is closely related to the traditional topic of projected gradient descent (PGD) \citep{chenFastLowrankEstimation2015, chenNonConvexProjectedGradient2019}. 
A key difference is that, \lowrank{} considers the specific gradient form that naturally appears in training multi-layer neural networks (e.g., it is a matrix with specific structures), proving many of its properties (e.g., Lemma~\ref{lemma:gradientlowrank}, Theorem~\ref{thm:gradientreversible}, and Theorem~\ref{thm:convgpg}). In contrast, traditional PGD mostly treats the objective as a general blackbox nonlinear function, and study the gradients in the vector space only. 

\paragraph{Low-rank gradient.}
Gradient is naturally low-rank during training of neural networks, and this property have been studied in both theory and practice \citep{zhaoZerOInitializationInitializing2022,cossonLowRankGradientDescent2023,yang2023spectral}.
It has been applied to reduce communication cost \citep{wangATOMOCommunicationefficientLearning,vogelsPowerGossipPracticalLowRank2020}, and memory footprint during training \citep{gooneratneLowrankGradientApproximation2020,huangLowRankGradientDescent2023,modoranuErrorFeedbackCan2024}.

\paragraph{Memory-efficient optimization.}
There have been some works trying to reduce the memory cost of gradient statistics for adaptive optimization algorithms \citep{shazeerAdafactorAdaptiveLearning,anilMemoryEfficientAdaptive,dettmers8bitOptimizersBlockwise2021}. 
Quantization is widely used to reduce the memory cost of optimizer states \citep{dettmers8bitOptimizersBlockwise2021,liMemoryEfficientOptimizers2023}.
Recent works have also proposed to reduce weight gradient memory by fusing the backward operation with the optimizer update \citep{lvAdaLomoLowmemoryOptimization2023,lvFullParameterFinetuning2023}.


