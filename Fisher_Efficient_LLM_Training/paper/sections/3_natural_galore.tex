\def\cG{\mathcal{G}}
\def\rr{\mathbb{R}}

\section{Accelerating GaLore with Second-Order Information}

Natural gradient methods are optimization algorithms that adjust parameter updates according to the geometry of the parameter space, leading to faster convergence compared to standard gradient descent \citep{amariNaturalGradientWorks1998}. They precondition the gradient using the inverse of the Fisher Information Matrix (FIM), effectively incorporating second-order information about the loss landscape. However, computing and storing the full FIM and its inverse is computationally infeasible for large-scale models due to their high dimensionality.

To address this challenge, we propose \textbf{Natural GaLore}, an online natural gradient algorithm that operates in a low-rank subspace of the gradient space. By projecting gradients onto this subspace and approximating the FIM within it, we efficiently compute natural gradient updates without explicit layer-wise information or significant computational overhead.

\subsection{Problem Setup}

Assume that the training/finetuning problem can be formulated as the following loss minimization problem: $$\min_{\theta} \Phi(\theta)$$ where $\theta$ is the model parameter and $\Phi$ is the loss function. The gradient of the loss function with respect to the model parameter is denoted as $\mathbf{g} = \nabla_{\theta} \Phi(\theta)$. The goal is to find an optimal update direction $\mathbf{\delta}$ that minimizes the loss function. Now in the case of GaLore, the update direction can be parameterized as: $\mathbf{\delta} = \mathbf{P^T u}$, where $\mathbf{P}$ is the projection matrix, which means that the Taylor series expansion for that update is given by: $$\Phi(\theta + \mathbf{P^T u}) \approx \Phi(\theta) + \mathbf{g^T P^T u} + \frac{1}{2} \mathbf{u^T PHP^T  u}$$ where $\mathbf{H} = \nabla^2_{\theta} \Phi(\theta)$ is the Hessian Matrix. Then the minimizer w.r.t. the low rank update direction $\mathbf{u}$ is given by: $$\mathbf{u^*} = \argmin_{u\in \mathbb{R}^r} \{\Phi(\theta_t) + \mathbf{g^T P^T u} + \frac{1}{2} \mathbf{u^T PHP^T  u}\}$$ Then the gradient descent update is given by: $$\theta_{t+1} = \theta_t + \mathbf{P^T u^*}$$.

In GaLore, the projection matrix $\mathbf{P}$ is chosen to be the top $r$ singular vectors of the gradient matrix $\mathbf{g}$ and the low rank update direction is chosen using Adam or AdamW optimizer. This method is memory efficient as it only requires storing the projection matrix $\mathbf{P}$ and costly optimizer states, now only depend upon the low rank update direction $\mathbf{u}$.

However, the performance of GaLore is still not on par with the vanilla AdamW optimizer. To bridge this gap, we propose Natural GaLore, which incorporates second-order information in the optimization process.

\subsection{Algorithm}

Our algorithm consists of the following key steps:

\begin{enumerate}
    \item \textbf{Low-Rank Gradient Projection}
    \item \textbf{Gradient History Buffer Maintenance}
    \item \textbf{FIM Approximation Using Gradient History}
    \item \textbf{Natural Gradient Computation via Woodbury Identity}
    \item \textbf{Efficient Solution Using Cholesky Decomposition}
\end{enumerate}

We explain each step in detail below.

\paragraph{Step 1: Low-Rank Gradient Projection}

\input{algos/project.tex}

Given a full-rank gradient tensor $\mathbf{g} \in \mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$, we project it onto a low-rank subspace using Tucker decomposition \citep{tuckerMathematicalNotesThree1966}. The Tucker decomposition approximates $\mathbf{g}$ as:

\[
\mathbf{g} \approx \mathcal{G} = \mathcal{C} \times_1 \mathbf{U}^{(1)} \times_2 \mathbf{U}^{(2)} \times_3 \cdots \times_d \mathbf{U}^{(d)},
\]

where:

\begin{itemize}
    \item $\mathcal{C} \in \mathbb{R}^{r_1 \times r_2 \times \cdots \times r_d}$ is the core tensor capturing the interaction between different modes.
    \item $\mathbf{U}^{(i)} \in \mathbb{R}^{n_i \times r_i}$ are the factor matrices (often orthogonal), representing the principal components along each mode.
    \item $\times_i$ denotes the mode-$i$ tensor-matrix product.
    \item $r_i$ is the rank along mode $i$, with $r_i \ll n_i$.
\end{itemize}

The transformed low-rank gradient is obtained by projecting $\mathbf{g}$ onto the subspace spanned by $\{\mathbf{U}^{(i)}\}$:

\[
\mathbf{g}_{\text{low}} = \text{Transform}(\mathbf{g}) = \mathbf{g} \times_1 (\mathbf{U}^{(1)})^\top \times_2 (\mathbf{U}^{(2)})^\top \times_3 \cdots \times_d (\mathbf{U}^{(d)})^\top.
\]

This results in a compact representation of the gradient in the low-rank subspace.

\paragraph{Step 2: Gradient History Buffer Maintenance}

\input{algos/natural_gradient.tex}

To capture local curvature information, we maintain a buffer of recent transformed gradients. Let $\mathbf{g}_t$ be the transformed low-rank gradient at iteration $t$, flattened into a vector:

\[
\mathbf{g}_t \in \mathbb{R}^{k}, \quad \text{where } k = r_1 r_2 \cdots r_d.
\]

We store the recent $s$ gradients in the matrix $\mathbf{G}$:

\[
\mathbf{G} = [\mathbf{g}_{t - s + 1}, \mathbf{g}_{t - s + 2}, \dots, \mathbf{g}_t] \in \mathbb{R}^{k \times s}.
\]

This gradient history captures the directions of recent updates, which are informative for approximating the FIM.

\paragraph{Step 3: Approximating the Fisher Information Matrix}

We approximate the Fisher Information Matrix within the low-rank subspace using the outer products of the stored gradients:

\[
\mathbf{F} = \lambda \mathbf{I}_k + \mathbf{G} \mathbf{G}^\top,
\]

where:

\begin{itemize}
    \item $\lambda > 0$ is a damping (regularization) term to ensure numerical stability.
    \item $\mathbf{I}_k$ is the $k \times k$ identity matrix.
\end{itemize}

This approximation is motivated by the empirical Fisher Information Matrix, which can be estimated using gradients observed during training \citep{martensNewPerspectiveNatural2014}.

\paragraph{Step 4: Computing the Natural Gradient via Woodbury Identity}

Computing $\mathbf{F}^{-1}$ directly is computationally expensive for large $k$. We employ the \textbf{Woodbury identity} to compute the inverse efficiently:

\[
\mathbf{F}^{-1} \mathbf{g}_t = \frac{1}{\lambda} \left( \mathbf{g}_t - \mathbf{G} \left( \lambda \mathbf{I}_s + \mathbf{G}^\top \mathbf{G} \right)^{-1} \mathbf{G}^\top \mathbf{g}_t \right).
\]

Defining:

\begin{itemize}
    \item $\mathbf{S} = \lambda \mathbf{I}_s + \mathbf{G}^\top \mathbf{G} \in \mathbb{R}^{s \times s}$.
    \item $\mathbf{y} = \mathbf{G}^\top \mathbf{g}_t \in \mathbb{R}^s$.
\end{itemize}

We compute the natural gradient as:

\begin{enumerate}
    \item Compute $\mathbf{S} = \lambda \mathbf{I}_s + \mathbf{G}^\top \mathbf{G}$.
    \item Solve $\mathbf{S} \mathbf{z} = \mathbf{y}$ for $\mathbf{z}$.
    \item Compute $\tilde{\mathbf{g}}_t = \dfrac{1}{\lambda} \left( \mathbf{g}_t - \mathbf{G} \mathbf{z} \right)$.
\end{enumerate}

This avoids inverting the large matrix $\mathbf{F}$ directly, reducing computational complexity.

\paragraph{Step 5: Efficient Solution Using Cholesky Decomposition}

To solve the linear system $\mathbf{S} \mathbf{z} = \mathbf{y}$ efficiently, we use \textbf{Cholesky decomposition}:

\begin{enumerate}
    \item Compute the Cholesky factorization of $\mathbf{S}$:

    \[
    \mathbf{S} = \mathbf{L} \mathbf{L}^\top,
    \]

    where $\mathbf{L}$ is a lower triangular matrix.

    \item Solve for $\mathbf{u}$ via forward substitution:

    \[
    \mathbf{L} \mathbf{u} = \mathbf{y}.
    \]

    \item Solve for $\mathbf{z}$ via backward substitution:

    \[
    \mathbf{L}^\top \mathbf{z} = \mathbf{u}.
    \]
\end{enumerate}

Cholesky decomposition is efficient for symmetric positive-definite matrices and numerically stable.

\paragraph{Step 6: Updating the Model Parameters}

\input{algos/project_back.tex}

Finally, we use the natural gradient $\tilde{\mathbf{g}}_t$ to update the model parameters in the low-rank subspace. The full-rank update can be reconstructed using the inverse transformation if necessary.

\subsection{Algorithm Summary}

At each iteration $t$, the algorithm proceeds as follows:

\begin{enumerate}
    \item \textbf{Project the Full-Rank Gradient}:

    \begin{itemize}
        \item Compute the low-rank transformed gradient $\mathbf{g}_t$ using the current factor matrices $\{\mathbf{U}^{(i)}\}$.
    \end{itemize}

    \item \textbf{Update Gradient History}:

    \begin{itemize}
        \item If the history buffer is full, remove the oldest gradient.
        \item Append $\mathbf{g}_t$ to the history buffer $\mathbf{G}$.
    \end{itemize}

    \item \textbf{Compute Intermediate Quantities}:

    \begin{itemize}
        \item $\mathbf{S} = \lambda \mathbf{I}_s + \mathbf{G}^\top \mathbf{G}$.
        \item $\mathbf{y} = \mathbf{G}^\top \mathbf{g}_t$.
    \end{itemize}

    \item \textbf{Solve for $\mathbf{z}$}:

    \begin{itemize}
        \item Use Cholesky decomposition to solve $\mathbf{S} \mathbf{z} = \mathbf{y}$.
    \end{itemize}

    \item \textbf{Compute the Natural Gradient}:

    \begin{itemize}
        \item $\tilde{\mathbf{g}}_t = \dfrac{1}{\lambda} \left( \mathbf{g}_t - \mathbf{G} \mathbf{z} \right)$.
    \end{itemize}

    \item \textbf{Update the Parameters}:

    \begin{itemize}
        \item Use $\tilde{\mathbf{g}}_t$ to update the model parameters in the low-rank subspace.
        \item Optionally, reconstruct the full gradient if necessary for parameter updates.
    \end{itemize}
\end{enumerate}

\subsection{Implementation Details}

\subsubsection{Avoiding Large Matrix Inversions}

By utilizing the Woodbury identity, we reduce the inversion of a large $k \times k$ matrix to the inversion of a much smaller $s \times s$ matrix, where $s$ (the history size) is typically small (e.g., $s = 10$). This makes the computation tractable even for large models.

\subsubsection{Computational Efficiency}

\begin{itemize}
    \item \textbf{Matrix-Vector Products}: We prioritize matrix-vector operations over matrix-matrix multiplications to enhance computational efficiency on GPUs.
    \item \textbf{GPU Acceleration}: All computations are performed on GPUs to leverage parallel processing capabilities and minimize data transfer overhead.
    \item \textbf{Optimized Linear Algebra Libraries}: We employ optimized GPU routines (e.g., cuBLAS, cuSOLVER) for linear algebra operations like matrix multiplication and Cholesky decomposition.
\end{itemize}

\subsubsection{Memory Efficiency}

The additional memory overhead is minimal due to:

\begin{itemize}
    \item \textbf{Small History Size}: Keeping $s$ small limits the size of $\mathbf{G}$ and related matrices.
    \item \textbf{Low-Rank Representation}: Operating in the low-rank subspace significantly reduces the dimensionality of the gradients and the FIM approximation.
\end{itemize}

\subsubsection{Numerical Stability}

The damping term $\lambda$ ensures that $\mathbf{S}$ remains positive definite, which is crucial for the stability of Cholesky decomposition. In practice, $\lambda$ can be treated as a hyperparameter, often set to a small positive value.

\subsubsection{Hyperparameter Selection}

Key hyperparameters include:

\begin{itemize}
    \item \textbf{Rank Parameters} $r_i$: Determines the dimensionality reduction in each mode; chosen based on a trade-off between computational cost and approximation accuracy.
    \item \textbf{History Size} $s$: Controls the amount of curvature information captured; typically small to balance memory usage and performance.
    \item \textbf{Damping Term} $\lambda$: Affects numerical stability and convergence; may require tuning for different models or datasets.
\end{itemize}

\subsection{Theoretical Justification}

Our method leverages the observation that gradients in deep learning often reside in a low-dimensional subspace due to the inherent redundancy in neural network parameterizations and correlations in the data \citep{gurariReducingTrainingTime2021}. By maintaining a history of low-rank gradients, we capture the principal components that approximate the local curvature of the loss surface.

Incorporating second-order information via the inverse FIM improves convergence rates, especially in regimes with limited iteration budgets. The use of the Woodbury identity allows us to efficiently compute the natural gradient in this low-rank subspace, effectively approximating the benefits of full natural gradient methods without their prohibitive computational costs.

\subsection{Empirical Evaluation}

We validate the effectiveness of Natural GaLore through empirical pre-training on LLaMA models with 60M, 300M, and 1.1B parameters using the C4 dataset \citep{raffelExploringLimitsTransfer2020}. Our experiments demonstrate that:

\begin{itemize}
    \item \textbf{Improved Convergence}: Natural GaLore achieves significantly lower perplexity compared to GaLore, indicating faster convergence.
    \item \textbf{Memory Efficiency}: The method incurs no additional memory overhead compared to GaLore, maintaining the advantages of low memory usage.
    \item \textbf{Performance Parity with Standard Optimizers}: The gap between our method and standard optimizers like Adam or AdamW is narrowed, demonstrating competitive performance.
\end{itemize}

\subsection{Conclusion}

We have introduced Natural GaLore, an efficient online natural gradient algorithm that operates in a low-rank subspace of the gradient space. By approximating the inverse Empirical Fisher Information Matrix using the Woodbury identity and maintaining a history of low-rank gradients, we incorporate second-order information into the optimization process without significant computational or memory overhead. Our method addresses the limitations of GaLore by improving convergence rates and achieving performance closer to that of standard optimizers, making it suitable for training large-scale language models under memory constraints.


\section{\lowrank: Gradient Low-Rank Projection}
\subsection{Background}
\input{figures/framework.tex}
\textbf{Regular full-rank training.} At time step $t$, $G_t = -\nabla_W \phi_t(W_t) \in \rr^{m \times n}$ is the backpropagated (negative) gradient matrix. Then the regular pre-training weight update can be written down as follows ($\eta$ is the learning rate):
\begin{equation}
    W_T = W_0 + \eta \sum_{t=0}^{T-1} \tilde G_{t} = W_0 + \eta\sum_{t=0}^{T-1} \rho_t(G_t)
\end{equation}
where $\tilde G_t$ is the final processed gradient to be added to the weight matrix and $\rho_t$ is an entry-wise stateful gradient regularizer (e.g., Adam). The state of $\rho_t$ can be memory-intensive. For example, for Adam, we need $M,V \in \rr^{m\times n}$ to regularize the gradient $G_t$ into $\tilde G_{t}$:
\begin{eqnarray}
    M_t &=& \beta_1 M_{t-1} + (1-\beta_1) G_t \\
    V_t &=& \beta_2 V_{t-1} + (1-\beta_2) G^2_t  \\
    \tilde G_t &=& M_t / \sqrt{V_t + \epsilon}
\end{eqnarray}
Here $G_t^2$ and $M_t / \sqrt{V_t + \epsilon}$ means element-wise multiplication and division. $\eta$ is the learning rate. Together with $W\in \rr^{m\times n}$, this takes $3mn$ memory.

From the theoretical analysis above, we can see that for batch size $N$, the gradient $G$ has certain structures: $G = \frac{1}{N}\sum_{i=1}^N (A_i - B_i W C_i)$ for input-dependent matrix $A_i$, Positive Semi-definite (PSD) matrices $B_i$ and $C_i$. In the following, we prove that such a gradient will become low-rank during training in certain conditions:

\def\sr{\mathrm{sr}}

\begin{restatable}[Gradient becomes low-rank during training]{lemma}{gradientlowrank}
\label{lemma:gradientlowrank}
    Suppose the gradient follows the parametric form:
    \begin{eqnarray}
          G_t=\frac{1}{N}\sum_{i=1}^N (A_i-B_i W_t C_i)\label{eq:constantgradientcoeff}
    \end{eqnarray}
    with constant $A_i$, PSD matrices $B_i$ and $C_i$ after $t \ge t_0$. We study vanilla SGD weight update: $W_t=W_{t-1}+\eta G_{t-1}$. Let $S := \frac{1}{N}\sum_{i=1}^N C_i \otimes B_i$ and $\lambda_1 < \lambda_2$ its two smallest distinct eigenvalues. Then the stable rank $\sr(G_t)$ satisfies:
    \begin{equation}
        \sr(G_t) \le \sr(\gzeroproj)\!+\!\left(\frac{1\!-\!\eta \lambda_2}{1\!-\!\eta \lambda_1}\right)^{2(t-t_0)} \frac{\|G_0\!-\!\gzeroproj\|_F^2}{\|\gzeroproj\|_2^2} \label{eq:stable-rank-decay}
    \end{equation}
    where $\gzeroproj$ is the projection of $G_{t_0}$ onto the minimal eigenspace $\cV_1$ of $S$ corresponding to $\lambda_1$.
\end{restatable}

In practice, the constant assumption can approximately hold for some time, in which the second term in Eq.~\ref{eq:stable-rank-decay} goes to zero exponentially and the stable rank of $G_t$ goes down, yielding low-rank gradient $G_t$. The final stable rank is determined by $\sr(\gzeroproj)$, which is estimated to be low-rank by the following:
\begin{restatable}[Low-rank $G_t$]{corollary}{lowrankmid}
\label{co:low-rank-mid}
If the gradient takes the parametric form $G_t = \frac{1}{N}\sum_{i=1}^N (\va_i - B_i W_t \vf_i)\vf_i^\top$ with all $B_i$ full-rank, and $N' := \rank(\{\vf_i\}) < n$, then $\sr(\gzeroproj) \le n - N'$ and thus $\sr(G_t) \le n/2$ for large $t$.
\end{restatable}

\textbf{Transformers.} For Transformers, we can also separately prove that the weight gradient of the lower layer (i.e., \emph{project-up}) weight of feed forward network (FFN) becomes low rank over time, using the JoMA framework~\cite{tian2023joma}. Please check Appendix (Sec.~\ref{sec:transformer-low-rank}) for details.

\section{\lowrank{} for Memory-Efficient Training}
For a complex optimization problem such as LLM pre-training, it may be difficult to capture the entire gradient trajectory with a single low-rank subspace. One reason is that the principal subspaces of $B_t$ and $C_t$ (and thus $G_t$) may change over time. In fact, if we keep the same projection $P$ and $Q$, then the learned weights will only grow along these subspaces, which is not longer full-parameter training. Fortunately, for this, \lowrank{} can switch subspaces during training and learn full-rank weights without increasing the memory footprint.

\textbf{Reducing memory footprint of gradient statistics.} \lowrank{} significantly reduces the memory cost of optimizer that heavily rely on component-wise gradient statistics, such as Adam \citep{kingmaAdamMethodStochastic2014}.

\subsection{Combining with Existing Techniques}

\lowrank{} is compatible with existing memory-efficient optimization techniques. For example, \lowrank{} can be combined with gradient checkpointing \citep{chenTrainingDeepNets2016} to further reduce memory usage.


\subsection{Hyperparameters of \lowrank{}}
\label{sec:lowrank-hyperparams}
In addition to Adam's original hyperparameters, \lowrank{} only introduces very few additional hyperparameters: the rank $r$ which is also present in LoRA, the subspace change frequency $T$ (see Sec.~\ref{sec:composition-subspace}), and the scale factor $\alpha$.

Scale factor $\alpha$ controls the strength of the low-rank update, which is similar to the scale factor $\alpha/r$ appended to the low-rank adaptor in \citet{huLoRALowRankAdaptation2021}.
We note that the $\alpha$ does not depend on the rank $r$ in our case.
This is because, when $r$ is small during pre-training, $\alpha/r$ significantly affects the convergence rate, unlike fine-tuning.

\input{tables/lora_compare.tex}
