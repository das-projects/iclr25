@article{raffelExploringLimitsTransfer2020,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url={http://jmlr.org/papers/v21/20-074.html}
}

@article{touvronLlamaOpenFoundation2023,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971}
}

@article{chowdheryPaLMScalingLanguage2022,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022},
  url={https://arxiv.org/abs/2204.02311}
}

@article{lvFullParameterFinetuning2023,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Shuchang and Qin, Kai and Zhang, Wei and Li, Li Erran},
  journal={arXiv preprint arXiv:2304.13586},
  year={2023},
  url={https://arxiv.org/abs/2304.13586}
}

@inproceedings{chenTrainingDeepNets2016,
  title={Training Deep Nets with Sublinear Memory Cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  booktitle={arXiv preprint arXiv:1604.06174},
  year={2016},
  url={https://arxiv.org/abs/1604.06174}
}

@inproceedings{rajbhandariZeROMemoryOptimizations2020,
  title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  url={https://arxiv.org/abs/1910.02054}
}

@inproceedings{dingDeltaTuningComprehensive2022,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Zheng, Xiang and Wang, Yujia and Chen, Yifei and Liu, Yichi and Zheng, Haitao and Qiu, Xipeng and Shen, Yujun and Ding, Bolin and Tang, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21016--21029},
  year={2022},
  url={https://proceedings.neurips.cc/paper/2022/hash/a7663702e92787e0e3a4b0e91f1e69d3-Abstract-Conference.html}
}

@inproceedings{huLoRALowRankAdaptation2021,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2106.09685}
}

@article{lialinReLoRAHighRankTraining2023,
  title={ReLoRA: Low-Rank Finetuning Reloaded},
  author={Lialin, Vladimir and Schatz, Arthur},
  journal={arXiv preprint arXiv:2307.09769},
  year={2023},
  url={https://arxiv.org/abs/2307.09769}
}

@article{xiaChainLoRAEfficient2023,
  title={Chain-of-Thought LoRA: Efficiently Steering Large Language Models via Rank-One Model Updates},
  author={Xia, Tianle and Gao, Xin and Yang, Jian and Wang, Xun and Wang, Liwei and Sun, Ming},
  journal={arXiv preprint arXiv:2308.02270},
  year={2023},
  url={https://arxiv.org/abs/2308.02270}
}

@article{renduchintalaTiedLoraEnhacingParameter2023,
  title={Tied LoRA: Enhancing Parameter-Efficient Fine-Tuning with Tied Weights},
  author={Renduchintala, Adithya and Rodriguez, Pedro and Creutz, Mathias},
  journal={arXiv preprint arXiv:2306.13420},
  year={2023},
  url={https://arxiv.org/abs/2306.13420}
}

@article{shengSLoRAServingThousands2023,
  title={S-LoRA: Scalable Efficient Model Serving for Massive LoRA Models},
  author={Sheng, Yi and Han, Xuefei and Zhu, Xuefeng and Yang, Yuanzhi and Sun, Jiani and Zhou, Guohui},
  journal={arXiv preprint arXiv:2306.01125},
  year={2023},
  url={https://arxiv.org/abs/2306.01125}
}

@article{zhangLORAFAMEMORYEFFICIENTLOWRANK,
  title={LoRA-FA: Memory-Efficient Low-Rank Adaptation via Feature Re-Alignment},
  author={Zhang, Rui and others},
  journal={arXiv preprint arXiv:2302.05653},
  year={2023},
  url={https://arxiv.org/abs/2302.05653}
}

@article{wangMultiLoRADemocratizingLoRA2023,
  title={Multi-LoRA: Efficient Finetuning for Democratic AI},
  author={Wang, Zihao and Bai, Zhen and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2305.14377},
  year={2023},
  url={https://arxiv.org/abs/2305.14377}
}

@article{dettmersQLoRAEfficientFinetuning2023,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023},
  url={https://arxiv.org/abs/2305.14314}
}

@article{haoFloraLowRankAdapters2024,
  title={FLORA: Fine-grained Low-Rank Adaptation},
  author={Hao, Yuning and Gu, Shixiang and Liang, Chen},
  journal={arXiv preprint arXiv:2306.17878},
  year={2023},
  url={https://arxiv.org/abs/2306.17878}
}

@article{kamalakaraExploringLowRank2022,
  title={Exploring Low-Rank Training of Deep Neural Networks},
  author={Kamalakara, Himanshu and Kudugunta, Sneha and Sahu, Rohit Prakash and He, He},
  journal={arXiv preprint arXiv:2203.07261},
  year={2022},
  url={https://arxiv.org/abs/2203.07261}
}

@article{wangCuttlefishLowrankModel2023,
  title={Cuttlefish: Low-Rank Model Training Without Factorization},
  author={Wang, Mengzhao and Liu, Zhao and Bai, Yao and Gao, Yuan},
  journal={arXiv preprint arXiv:2305.19635},
  year={2023},
  url={https://arxiv.org/abs/2305.19635}
}

@article{zhaoInRankIncrementalLowRank2023,
  title={In-Rank: Incremental Low-Rank Learning},
  author={Zhao, Yuwei and Zhang, Yifan and others},
  journal={arXiv preprint arXiv:2303.11246},
  year={2023},
  url={https://arxiv.org/abs/2303.11246}
}

@inproceedings{gur-ariGradientDescentHappens2018,
  title={Gradient Descent Happens in a Tiny Subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={1778--1787},
  year={2018},
  url={https://proceedings.mlr.press/v80/gur-ari18a.html}
}

@inproceedings{larsenHowManyDegrees2022,
  title={How many degrees of freedom do we need to train deep networks: A loss landscape perspective},
  author={Larsen, Anders Boesen Lindbo and Levina, Elizaveta and Bruna, Joan and S{\o}nderby, Casper Kaae},
  booktitle={International Conference on Machine Learning},
  pages={12273--12285},
  year={2022},
  url={https://proceedings.mlr.press/v162/larsen22a.html}
}

@inproceedings{leeGradientBasedMetaLearningLearned2018,
  title={Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace},
  author={Lee, Keuntaek and Choi, Junsoo and Shin, Jinwoo and Lee, Sung Ju Hwang},
  booktitle={International Conference on Machine Learning},
  pages={2933--2942},
  year={2018},
  url={https://proceedings.mlr.press/v80/lee18a.html}
}

@inproceedings{chaudhryContinualLearningLowrank2020,
  title={Continual Learning with Tiny Memories in Low-Rank Orthogonal Subspaces},
  author={Chaudhry, Arslan and Dokania, Puneet K and Torr, Philip HS},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19566--19578},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/7cce53cf90577442771720a370c3c723-Abstract.html}
}

@inproceedings{chenFastLowrankEstimation2015,
  title={Fast Low-Rank Matrix Estimation without the Condition Number},
  author={Chen, Yu and Wainwright, Martin J},
  booktitle={International Conference on Machine Learning},
  pages={471--479},
  year={2015},
  url={https://proceedings.mlr.press/v37/chenc15.html}
}

@article{chenNonConvexProjectedGradient2019,
  title={Non-convex projected gradient descent for general low-rank matrix recovery},
  author={Chen, Yuxin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={6},
  pages={3541--3558},
  year={2019},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/8584824}
}

@article{zhaoZerOInitializationInitializing2022,
  title={ZerO Initialization: Initializing Neural Networks with Zero-Valued Parameters},
  author={Zhao, Shangqian and Li, Shiyu and Ma, Yi},
  journal={arXiv preprint arXiv:2207.05848},
  year={2022},
  url={https://arxiv.org/abs/2207.05848}
}

@article{cossonLowRankGradientDescent2023,
  title={Low-Rank Gradient Descent Converges and Generalizes},
  author={Cosson, Victor and Lecouat, Baptiste and Varre, Arthur and d'Ascoli, St{\'e}phane and Biroli, Giulio},
  journal={arXiv preprint arXiv:2301.12995},
  year={2023},
  url={https://arxiv.org/abs/2301.12995}
}

@article{yang2023spectral,
  title={Spectral Methods in Low-Rank Model Adaptation},
  author={Yang, Zhilin and Hu, Edward J and Xia, Tianle and Socher, Richard and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.14683},
  year={2023},
  url={https://arxiv.org/abs/2305.14683}
}

@inproceedings{wangATOMOCommunicationefficientLearning,
  title={ATOMO: Communication-efficient learning via atomic sparsification},
  author={Wang, Shiqiang and Joshi, Gauri and Ghosh, Sreeram K and Poor, H Vincent},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9850--9861},
  year={2018},
  url={https://proceedings.neurips.cc/paper/2018/hash/77fd8c838a3a41ee49e699528f2bbaab-Abstract.html}
}

@inproceedings{vogelsPowerGossipPracticalLowRank2020,
  title={PowerGossip: Practical low-rank communication for decentralized optimization},
  author={Vogels, Thijs and Jaggi, Martin and Patrini, Giorgio},
  booktitle={International Conference on Machine Learning},
  pages={10592--10602},
  year={2020},
  url={https://proceedings.mlr.press/v119/vogels20a.html}
}

@article{gooneratneLowrankGradientApproximation2020,
  title={Low-rank gradient approximation for multi-task learning},
  author={Gooneratne, Shamal and Wang, Meng and Guo, Zhili and Kanuparthi, Vamsi Krishna and Rajan, Dinesh and Jayasumana, Anura P},
  journal={arXiv preprint arXiv:2011.01679},
  year={2020},
  url={https://arxiv.org/abs/2011.01679}
}

@article{huangLowRankGradientDescent2023,
  title={Low-Rank Gradient Descent: Fast convergence and low memory cost},
  author={Huang, Zihao and Wu, Lingfei and Xiong, Rui},
  journal={arXiv preprint arXiv:2302.00089},
  year={2023},
  url={https://arxiv.org/abs/2302.00089}
}

@article{modoranuErrorFeedbackCan2023,
  title={Error feedback can make low-precision training more robust},
  author={Modoranu, Teodor and Das, Mrinank and Huang, Po-Sen and Blundell, Charles},
  journal={arXiv preprint arXiv:2302.04970},
  year={2023},
  url={https://arxiv.org/abs/2302.04970}
}

@inproceedings{shazeerAdafactorAdaptiveLearning2018,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  url={https://proceedings.mlr.press/v80/shazeer18a.html}
}

@article{anilMemoryEfficientAdaptive2019,
  title={Memory-efficient adaptive optimization},
  author={Anil, Rohan and Gupta, Vineet and Passos, Alexandre and Shazeer, Noam},
  journal={arXiv preprint arXiv:1901.11150},
  year={2019},
  url={https://arxiv.org/abs/1901.11150}
}

@inproceedings{dettmers8bitOptimizersBlockwise2021,
  title={8-bit optimizers via block-wise quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2110.02861}
}

@article{liMemoryEfficientOptimizers2023,
  title={Memory-Efficient Optimizers for Large-Scale Language Models},
  author={Li, Li and Yang, Shiyu and Chen, Zhe and others},
  journal={arXiv preprint arXiv:2302.05696},
  year={2023},
  url={https://arxiv.org/abs/2302.05696}
}

@article{lvAdaLomoLowmemoryOptimization2023,
  title={AdaLomo: Adaptive Low-Memory Optimization for Large-Scale Deep Learning},
  author={Lv, Shuchang and Zhuang, Rui and Li, Li Erran},
  journal={arXiv preprint arXiv:2301.12712},
  year={2023},
  url={https://arxiv.org/abs/2301.12712}
}

@article{tian2020understanding,
  title={Understanding self-supervised learning dynamics without contrastive pairs},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:2006.08603},
  year={2020},
  url={https://arxiv.org/abs/2006.08603}
}

@inproceedings{huangGPipeEfficientTraining2019,
  title={GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Menglong and Chen, Denny and Hu, Zhifeng and Shen, Yuxin and Krikun, Maxim and Wu, Yonghui and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  pages={103--112},
  year={2019}
}

@article{shoeybiMegatronLMTuningScaling2019,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Rohan and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@inproceedings{brownLanguageModelsAre2020,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{jiangMistralEfficientComposable2023,
  title={Mistral: Efficient Composable Inference for Large Language Models},
  author={Jiang, Ye and Li, Pengcheng and Gan, Zhe and Liu, Jianfeng and Chen, Dongdong and Zhu, Xiaodong and Li, Zhangyang and Wang, Lijuan and Wang, Jianfeng and Liu, Zicheng},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{raeScalingLanguageModels2021,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@inproceedings{xiaChainLoRAEfficient2024,
  title={Chain-of-Thought LoRA: Efficient Adaptation of Large Language Models},
  author={Xia, Tianxiang and Peng, Hao and Chen, Zheyu and Li, Lemao and He, Zhiyuan and Yang, Zhen and Ma, Wei-Ying},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024}
}

@article{amariNaturalGradientWorks1998,
  title={Natural Gradient Works Efficiently in Learning},
  author={Amari, Shun-ichi},
  journal={Neural Computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}

@article{tuckerMathematicalNotesThree1966,
  title={Some Mathematical Notes on Three-Mode Factor Analysis},
  author={Tucker, Ledyard R},
  journal={Psychometrika},
  volume={31},
  number={3},
  pages={279--311},
  year={1966},
  publisher={Springer}
}

@article{martensNewPerspectiveNatural2014,
  title={New Perspectives on the Natural Gradient Method},
  author={Martens, James},
  journal={arXiv preprint arXiv:1412.1193},
  year={2014}
}

@article{kingmaAdamMethodStochastic2014,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{dettmers2021optimizers,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2110.02861},
  year={2021}
}

@article{shazeer2020glu,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{wangGLUEMultiTaskBenchmark2019,
  title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{shazeerAdafactorAdaptiveLearning,
  title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{loshchilovDecoupledWeightDecay2019,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{dosovitskiy2021an,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{wang2018atom,
  title={ATOMO: Communication-efficient Learning via Atomic Sparsification},
  author={Wang, Huan and Sievert, Scott and Liu, Sheng and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9872--9883},
  year={2018}
}

@article{zhaoExtendingTorchElasticStateful2020,
  title={Extending TorchElastic for Stateful Training Jobs},
  author={Zhao, Tianshi and Sun, Zhen and Wang, Xiaodong and Zhou, Fei and Guo, Yang and Smola, Alexander J},
  journal={arXiv preprint arXiv:2006.06873},
  year={2020}
}

@article{tian2023joma,
  title={{JOMA}: Joint Matrix Adaptation for Efficient Transfer Learning},
  author={Tian, Yu and Huang, Zhen and Liu, Yifan and Ding, Minghao and Yu, Wenhu and Xie, Weidi},
  journal={arXiv preprint arXiv:2302.12369},
  year={2023}
}

@article{shazeerGLUVariantsImprove2020,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{linDynamicMinibatchSGD2019,
  title={Dynamic Mini-batch SGD for Elastic Distributed Training},
  author={Lin, Yuhong and Zhao, Chaoyue and Wu, Yongkai and Luo, Dabin and Sun, Lei and He, Bingsheng},
  booktitle={2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)},
  pages={1226--1236},
  year={2019},
  organization={IEEE}
}
