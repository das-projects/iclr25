\begin{table*}[t]
    \centering
    \caption{\small{Comparison of Natural GaLore with other low-rank algorithms on pre-training various sizes of LLaMA models on the C4 dataset. Validation perplexity is reported, along with a memory estimate (in gigabytes) of the total parameters and optimizer states based on BF16 format. The actual memory footprint of GaLore is reported in Figure~\ref{fig:memory_vs_model_size}.}}
    \label{tab:lora_compare_llama}
    \begin{tabular}{lcccc}
    \toprule
                     & \textbf{60M} & \textbf{130M} & \textbf{350M} & \textbf{1.1B} \\
    \midrule
    Full-Rank        & 34.06 (0.36G) & 25.08 (0.76G) & 18.80 (2.06G) & 15.56 (7.80G) \\
    \midrule
    \lowrank & \textbf{34.20} (0.24G) & \textbf{25.12} (0.52G) & \textbf{18.85} (1.22G) & \textbf{15.58} (4.38G) \\
    GaLore           & 34.88 (0.24G) & 25.36 (0.52G) & 18.95 (1.22G) & 15.64 (4.38G) \\
    Low-Rank         & 78.18 (0.26G) & 45.51 (0.54G) & 37.41 (1.08G) & 142.53 (3.57G) \\
    LoRA             & 34.99 (0.36G) & 33.92 (0.80G) & 25.58 (1.76G) & 19.21 (6.17G) \\
    ReLoRA           & 37.04 (0.36G) & 29.37 (0.80G) & 29.08 (1.76G) & 18.33 (6.17G) \\
    \bottomrule
    Rank $r / d_{\text{model}}$ & 128 / 256 & 256 / 768 & 256 / 1024 & 512 / 2048 \\
    Training Tokens  & 1.1B & 2.2B & 6.4B & 13.1B \\
    \bottomrule
    \end{tabular}
\end{table*}