@article{raffelExploringLimitsTransfer2020,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url={http://jmlr.org/papers/v21/20-074.html}
}

@article{touvronLlamaOpenFoundation2023,
  title={{LLaMA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971}
}

@article{chowdheryPaLMScalingLanguage2022,
  title={{PaLM}: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung~Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022},
  url={https://arxiv.org/abs/2204.02311}
}

@article{lvFullParameterFinetuning2023,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Shuchang and Qin, Kai and Zhang, Wei and Li, Li Erran},
  journal={arXiv preprint arXiv:2304.13586},
  year={2023},
  url={https://arxiv.org/abs/2304.13586}
}

@inproceedings{chenTrainingDeepNets2016,
  title={Training Deep Nets with Sublinear Memory Cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML)},
  year={2016},
  url={https://arxiv.org/abs/1604.06174}
}

@inproceedings{rajbhandariZeROMemoryOptimizations2020,
  title={{ZeRO}: Memory Optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  url={https://arxiv.org/abs/1910.02054}
}

@inproceedings{dingDeltaTuningComprehensive2022,
  title={Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models},
  author={Ding, Ning and Zheng, Xiang and Wang, Yujia and Chen, Yifei and Liu, Yichi and Zheng, Haitao and Qiu, Xipeng and Shen, Yujun and Ding, Bolin and Tang, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21016--21029},
  year={2022},
  url={https://arxiv.org/abs/2203.06904}
}

@inproceedings{huLoRALowRankAdaptation2021,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2106.09685}
}

@article{lialinReLoRAHighRankTraining2023,
  title={{ReLoRA}: Low-Rank Fine-Tuning Reloaded},
  author={Lialin, Vladimir and Schatz, Arthur},
  journal={arXiv preprint arXiv:2307.09769},
  year={2023},
  url={https://arxiv.org/abs/2307.09769}
}

@article{xiaChainLoRAEfficient2023,
  title={Chain-of-Thought LoRA: Efficiently Steering Large Language Models via Rank-One Model Updates},
  author={Xia, Tianle and Gao, Xin and Yang, Jian and Wang, Xun and Wang, Liwei and Sun, Ming},
  journal={arXiv preprint arXiv:2308.02270},
  year={2023},
  url={https://arxiv.org/abs/2308.02270}
}

@article{renduchintalaTiedLoraEnhacingParameter2023,
  title={Tied LoRA: Enhancing Parameter-Efficient Fine-Tuning with Tied Weights},
  author={Renduchintala, Adithya and Rodriguez, Pedro and Creutz, Mathias},
  journal={arXiv preprint arXiv:2306.13420},
  year={2023},
  url={https://arxiv.org/abs/2306.13420},
  note={\href{https://arxiv.org/abs/2306.13420}{arXiv}}
}

@article{shengSLoRAServingThousands2023,
  title={{S-LoRA}: Scalable Efficient Model Serving for Massive LoRA Models},
  author={Sheng, Yi and Han, Xuefei and Zhu, Xuefeng and Yang, Yuanzhi and Sun, Jiani and Zhou, Guohui},
  journal={arXiv preprint arXiv:2306.01125},
  year={2023},
  url={https://arxiv.org/abs/2306.01125}
}

@article{zhangLORAFAMEMORYEFFICIENTLOWRANK,
  title={{LoRA-FA}: Memory-Efficient Low-Rank Adaptation via Feature Re-Alignment},
  author={Zhang, Rui and others},
  journal={arXiv preprint arXiv:2302.05653},
  year={2023},
  url={https://arxiv.org/abs/2302.05653}
}

@article{wangMultiLoRADemocratizingLoRA2023,
  title={{Multi-LoRA}: Efficient Fine-Tuning for Democratic {AI}},
  author={Wang, Zihao and Bai, Zhen and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2305.14377},
  year={2023},
  url={https://arxiv.org/abs/2305.14377}
}

@article{dettmersQLoRAEfficientFinetuning2023,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023},
  url={https://arxiv.org/abs/2305.14314}
}

@article{haoFloraLowRankAdapters2024,
  title={{FLORA}: Fine-grained Low-Rank Adaptation},
  author={Hao, Yuning and Gu, Shixiang and Liang, Chen},
  journal={arXiv preprint arXiv:2306.17878},
  year={2023},
  url={https://arxiv.org/abs/2306.17878}
}

@article{kamalakaraExploringLowRank2022,
  title={Exploring Low-Rank Training of Deep Neural Networks},
  author={Kamalakara, Himanshu and Kudugunta, Sneha and Sahu, Rohit Prakash and He, He},
  journal={arXiv preprint arXiv:2203.07261},
  year={2022},
  url={https://arxiv.org/abs/2203.07261}
}

@article{wangCuttlefishLowrankModel2023,
  title={{Cuttlefish}: Low-Rank Model Training Without Factorization},
  author={Wang, Mengzhao and Liu, Zhao and Bai, Yao and Gao, Yuan},
  journal={arXiv preprint arXiv:2305.19635},
  year={2023},
  url={https://arxiv.org/abs/2305.19635}
}

@article{zhaoInRankIncrementalLowRank2023,
  title={{In-Rank}: Incremental Low-Rank Learning},
  author={Zhao, Yuwei and Zhang, Yifan and others},
  journal={arXiv preprint arXiv:2303.11246},
  year={2023},
  url={https://arxiv.org/abs/2303.11246}
}

@inproceedings{gur-ariGradientDescentHappens2018,
  title={Gradient Descent Happens in a Tiny Subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={1778--1787},
  year={2018},
  url={https://arxiv.org/abs/1812.04754}
}

@inproceedings{larsenHowManyDegrees2022,
  title={How Many Degrees of Freedom Do We Need to Train Deep Networks: A Loss Landscape Perspective},
  author={Larsen, Anders Boesen Lindbo and Levina, Elizaveta and Bruna, Joan and S{\o}nderby, Casper Kaae},
  booktitle={International Conference on Machine Learning},
  pages={12273--12285},
  year={2022},
  url={https://arxiv.org/abs/2107.05802}}
}

@inproceedings{leeGradientBasedMetaLearningLearned2018,
  title={Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace},
  author={Lee, Keuntaek and Choi, Junsoo and Shin, Jinwoo and Lee, Sung Ju Hwang},
  booktitle={International Conference on Machine Learning},
  pages={2933--2942},
  year={2018},
  url={https://arxiv.org/abs/1801.05558}
}

@inproceedings{chaudhryContinualLearningLowrank2020,
  title={Continual Learning with Tiny Memories in Low-Rank Orthogonal Subspaces},
  author={Chaudhry, Arslan and Dokania, Puneet K and Torr, Philip HS},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19566--19578},
  year={2020},
  url={https://arxiv.org/abs/2010.11635}
}

@inproceedings{chenFastLowrankEstimation2015,
  title={Fast Low-Rank Matrix Estimation without the Condition Number},
  author={Chen, Yu and Wainwright, Martin J},
  booktitle={International Conference on Machine Learning},
  pages={471--479},
  year={2017},
  url={https://arxiv.org/abs/1712.03281},
}

@article{chenNonConvexProjectedGradient2019,
  title={Non-Convex Projected Gradient Descent for General Low-Rank Matrix Recovery},
  author={Chen, Yuxin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={6},
  pages={3541--3558},
  year={2019},
  publisher={IEEE},
  url={https://doi.org/10.1109/TIT.2019.2899616}
}

@article{zhaoZerOInitializationInitializing2022,
  title={{ZerO} Initialization: Initializing Neural Networks with Zero-Valued Parameters},
  author={Zhao, Shangqian and Li, Shiyu and Ma, Yi},
  journal={arXiv preprint arXiv:2207.05848},
  year={2022},
  url={https://arxiv.org/abs/2207.05848}
}

@article{cossonLowRankGradientDescent2023,
  title={Low-Rank Gradient Descent Converges and Generalizes},
  author={Cosson, Victor and Lecouat, Baptiste and Varre, Arthur and d'Ascoli, St{\'e}phane and Biroli, Giulio},
  journal={arXiv preprint arXiv:2301.12995},
  year={2023},
  url={https://arxiv.org/abs/2301.12995}
}

@article{yang2023spectral,
  title={Spectral Methods in Low-Rank Model Adaptation},
  author={Yang, Zhilin and Hu, Edward J and Xia, Tianle and Socher, Richard and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.14683},
  year={2023},
  url={https://arxiv.org/abs/2305.14683}
}

@inproceedings{wangATOMOCommunicationefficientLearning,
  title={{ATOMO}: Communication-Efficient Learning via Atomic Sparsification},
  author={Wang, Shiqiang and Joshi, Gauri and Ghosh, Sreeram K and Poor, H Vincent},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9850--9861},
  year={2018},
  url={https://arxiv.org/abs/1806.04090}
}

@inproceedings{vogelsPowerGossipPracticalLowRank2020,
  title={{PowerGossip}: Practical Low-Rank Communication for Decentralized Optimization},
  author={Vogels, Thijs and Jaggi, Martin and Patrini, Giorgio},
  booktitle={International Conference on Machine Learning},
  pages={10592--10602},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/a376802c0811f1b9088828288eb0d3f0-Paper.pdf}
}

@article{gooneratneLowrankGradientApproximation2020,
  title={Low-Rank Gradient Approximation for Multi-Task Learning},
  author={Gooneratne, Shamal and Wang, Meng and Guo, Zhili and Kanuparthi, Vamsi Krishna and Rajan, Dinesh and Jayasumana, Anura P},
  journal={arXiv preprint arXiv:2011.01679},
  year={2020},
  url={https://arxiv.org/abs/2011.01679}
}

@article{huangLowRankGradientDescent2023,
  title={GaLore: Low-Rank Gradient Descent: Fast Convergence and Low Memory Cost},
  author={Zhao, Jiawei and Zhang, Zhenyu and others},
  journal={International Conference on Machine Learning},
  year={2024},
  url={https://arxiv.org/abs/2403.03507v2}
}

@article{modoranuErrorFeedbackCan2023,
  title={Error Feedback Can Make Low-Precision Training More Robust},
  author={Modoranu, Teodor and Das, Mrinank and Huang, Po-Sen and Blundell, Charles},
  journal={arXiv preprint arXiv:2302.04970},
  year={2023},
  url={https://arxiv.org/abs/2302.04970}
}

@inproceedings{shazeerAdafactorAdaptiveLearning2018,
  title={{Adafactor}: Adaptive Learning Rates with Sublinear Memory Cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  url={https://arxiv.org/abs/1804.04235}
}

@article{anilMemoryEfficientAdaptive2019,
  title={Memory-Efficient Adaptive Optimization},
  author={Anil, Rohan and Gupta, Vineet and Passos, Alexandre and Shazeer, Noam},
  journal={arXiv preprint arXiv:1901.11150},
  year={2019},
  url={https://arxiv.org/abs/1901.11150}
}

@inproceedings{dettmers8bitOptimizersBlockwise2021,
  title={8-bit Optimizers via Block-wise Quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2110.02861}
}

@article{liMemoryEfficientOptimizers2023,
  title={Memory-Efficient Optimizers for Large-Scale Language Models},
  author={Li, Li and Yang, Shiyu and Chen, Zhe and others},
  journal={arXiv preprint arXiv:2302.05696},
  year={2023},
  url={https://arxiv.org/abs/2302.05696}
}

@article{lvAdaLomoLowmemoryOptimization2023,
  title={{AdaLomo}: Adaptive Low-Memory Optimization for Large-Scale Deep Learning},
  author={Lv, Shuchang and Zhuang, Rui and Li, Li Erran},
  journal={arXiv preprint arXiv:2301.12712},
  year={2023},
  url={https://arxiv.org/abs/2301.12712}
}

@article{tian2020understanding,
  title={Understanding Self-Supervised Learning Dynamics Without Contrastive Pairs},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:2006.08603},
  year={2020},
  url={https://arxiv.org/abs/2006.08603}
}

@inproceedings{huangGPipeEfficientTraining2019,
  title={{GPipe}: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Menglong and Chen, Denny and Hu, Zhifeng and Shen, Yuxin and Krikun, Maxim and Wu, Yonghui and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  pages={103--112},
  year={2019},
  url={https://arxiv.org/abs/1811.06965}
}

@article{shoeybiMegatronLMTuningScaling2019,
  title={{Megatron-LM}: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Rohan and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019},
  url={https://arxiv.org/abs/1909.08053}
}

@inproceedings{brownLanguageModelsAre2020,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020},
  url={https://arxiv.org/abs/2005.14165}
}

@article{jiangMistralEfficientComposable2023,
  title={{Mistral}: Efficient Composable Inference for Large Language Models},
  author={Jiang, Ye and Li, Pengcheng and Gan, Zhe and Liu, Jianfeng and Chen, Dongdong and Zhu, Xiaodong and Li, Zhangyang and Wang, Lijuan and Wang, Jianfeng and Liu, Zicheng},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023},
  url={https://arxiv.org/abs/2305.15334}
}

@article{raeScalingLanguageModels2021,
  title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021},
  url={https://arxiv.org/abs/2112.11446}
}

@inproceedings{xiaChainLoRAEfficient2024,
  title={Chain-of-Thought LoRA: Efficient Adaptation of Large Language Models},
  author={Xia, Tianxiang and Peng, Hao and Chen, Zheyu and Li, Lemao and He, Zhiyuan and Yang, Zhen and Ma, Wei-Ying},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2024},
  note={To appear}
}

@article{amariNaturalGradientWorks1998,
  title={Natural Gradient Works Efficiently in Learning},
  author={Amari, Shun-ichi},
  journal={Neural Computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press},
  url={https://ieeexplore.ieee.org/document/6790500}
}

@article{tuckerMathematicalNotesThree1966,
  title={Some Mathematical Notes on Three-Mode Factor Analysis},
  author={Tucker, Ledyard R},
  journal={Psychometrika},
  volume={31},
  number={3},
  pages={279--311},
  year={1966},
  publisher={Springer},
  doi={10.1007/BF02289464}
}

@article{martensNewPerspectiveNatural2014,
  title={New Perspectives on the Natural Gradient Method},
  author={Martens, James},
  journal={arXiv preprint arXiv:1412.1193},
  year={2014},
  url={https://arxiv.org/abs/1412.1193}
}

@article{lin2022randomized,
  title={Randomized Subspace Regularized Newton Method for Unconstrained Non-Convex Optimization},
  author={Lin, Tianyi and Zhu, Zhihui and Mao, Yongyi},
  journal={arXiv preprint arXiv:2209.04170},
  year={2022},
  url={https://arxiv.org/abs/2209.04170}
}

@article{kingmaAdamMethodStochastic2014,
  title={{Adam}: A Method for Stochastic Optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014},
  url={https://arxiv.org/abs/1412.6980}
}

@article{shazeerGLUVariantsImprove2020,
  title={{GLU} Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020},
  url={https://arxiv.org/abs/2002.05202}
}

@inproceedings{wangGLUEMultiTaskBenchmark2019,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://arxiv.org/abs/1804.07461}
}

@article{loshchilovDecoupledWeightDecay2019,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017},
  url={https://arxiv.org/abs/1711.05101}
}

@article{dosovitskiy2021an,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020},
  url={https://arxiv.org/abs/2010.11929}
}

@inproceedings{ho2020denoising,
  title={Denoising Diffusion Probabilistic Models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020},
  url={https://arxiv.org/abs/2006.11239}
}

@article{zhaoExtendingTorchElasticStateful2020,
  title={Extending TorchElastic for Stateful Training Jobs},
  author={Zhao, Tianshi and Sun, Zhen and Wang, Xiaodong and Zhou, Fei and Guo, Yang and Smola, Alexander J},
  journal={arXiv preprint arXiv:2006.06873},
  year={2020},
  url={https://arxiv.org/abs/2006.06873}
}

@article{tian2023joma,
  title={{JOMA}: Joint Matrix Adaptation for Efficient Transfer Learning},
  author={Tian, Yu and Huang, Zhen and Liu, Yifan and Ding, Minghao and Yu, Wenhu and Xie, Weidi},
  journal={arXiv preprint arXiv:2302.12369},
  year={2023},
  url={https://arxiv.org/abs/2302.12369}
}

@inproceedings{linDynamicMinibatchSGD2019,
  title={Dynamic Mini-Batch {SGD} for Elastic Distributed Training},
  author={Lin, Yuhong and Zhao, Chaoyue and Wu, Yongkai and Luo, Dabin and Sun, Lei and He, Bingsheng},
  booktitle={2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS)},
  pages={1226--1236},
  year={2019},
  publisher={IEEE},
  doi={10.1109/ICDCS.2019.00123}
}

@article{martens2020new,
  title={New Insights and Perspectives on the Natural Gradient Method},
  author={Martens, James},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--76},
  year={2020},
  url={https://www.jmlr.org/papers/volume21/17-678/17-678.pdf}
}

@article{erdogan2024tinyagent,
  title={{TinyAgent}: Function Calling at the Edge},
  author={Erdogan, Lutfi Eren and Lee, Nicholas and Jha, Siddharth and Kim, Sehoon and Tabrizi, Ryan and Moon, Suhong and Hooper, Coleman and Anumanchipalli, Gopala and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2409.00608},
  year={2024},
  url={https://arxiv.org/abs/2409.00608}
}

@article{kim2023llmcompiler,
  title={An {LLM} Compiler for Parallel Function Calling},
  author={Kim, Sehoon and Moon, Suhong and Tabrizi, Ryan and Lee, Nicholas and Mahoney, Michael W and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2312.04511},
  year={2023},
  url={https://arxiv.org/abs/2312.04511}
}

@article{zhao2024galore,
  title={{GaLore}: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024},
  url={https://arxiv.org/abs/2403.03507}
}

@inproceedings{martens2015optimizing,
  title={Optimizing Neural Networks with Kronecker-Factored Approximate Curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  pages={2408--2417},
  year={2015},
  url={https://proceedings.mlr.press/v37/martens15.html}
}

@article{he2021debertav3,
  title={{DeBERTaV3}: Improving {DeBERTa} using {ELECTRA}-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2111.09543},
  year={2021},
  url={https://arxiv.org/abs/2111.09543}
}

@article{loshchilov2017decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017},
  url={https://arxiv.org/abs/1711.05101}
}

@article{warstadt2019neural,
  title={Neural Network Acceptability Judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={625--641},
  year={2019},
  doi={10.1162/tacl\_a\_00290},
  url={https://doi.org/10.1162/tacl_a_00290}
}

@inproceedings{dolan2005automatically,
  title={Automatically Constructing a Corpus of Sentential Paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005},
  url={https://aclanthology.org/I05-5002}
}

@inproceedings{cer2017semeval,
  title={SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, I{\~n}igo and Specia, Lucia},
  booktitle={Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
  pages={1--14},
  year={2017},
  doi={10.18653/v1/S17-2001},
  url={https://aclanthology.org/S17-2001}
}

@inproceedings{dagan2005pascal,
  title={The {PASCAL} Recognising Textual Entailment Challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Proceedings of the First International Conference on Machine Learning Challenges: Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment},
  pages={177--190},
  year={2006},
  publisher={Springer},
  doi={10.1007/11736790\_9},
  url={https://doi.org/10.1007/11736790_9}
}

@inproceedings{williams2018broad,
  title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  volume={1},
  pages={1112--1122},
  year={2018},
  doi={10.18653/v1/N18-1101},
  url={https://aclanthology.org/N18-1101}
}

@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016},
  doi={10.18653/v1/D16-1264},
  url={https://aclanthology.org/D16-1264}
}

@misc{yang2024largelanguagemodelsoptimizers,
      title={Large Language Models as Optimizers},
      author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
      year={2024},
      eprint={2309.03409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.03409},
}

@misc{opsahlong2024optimizinginstructionsdemonstrationsmultistage,
      title={Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs},
      author={Krista Opsahl-Ong and Michael J Ryan and Josh Purtell and David Broman and Christopher Potts and Matei Zaharia and Omar Khattab},
      year={2024},
      eprint={2406.11695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11695},
}