@article{raffelExploringLimitsTransfer2020,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020},
  url={http://jmlr.org/papers/v21/20-074.html}
}

@article{touvronLlamaOpenFoundation2023,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971}
}

@article{chowdheryPaLMScalingLanguage2022,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022},
  url={https://arxiv.org/abs/2204.02311}
}

@article{lvFullParameterFinetuning2023,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Shuchang and Qin, Kai and Zhang, Wei and Li, Li Erran},
  journal={arXiv preprint arXiv:2304.13586},
  year={2023},
  url={https://arxiv.org/abs/2304.13586}
}

@inproceedings{chenTrainingDeepNets2016,
  title={Training Deep Nets with Sublinear Memory Cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  booktitle={arXiv preprint arXiv:1604.06174},
  year={2016},
  url={https://arxiv.org/abs/1604.06174}
}

@inproceedings{rajbhandariZeROMemoryOptimizations2020,
  title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  url={https://arxiv.org/abs/1910.02054}
}

@inproceedings{dingDeltaTuningComprehensive2022,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Zheng, Xiang and Wang, Yujia and Chen, Yifei and Liu, Yichi and Zheng, Haitao and Qiu, Xipeng and Shen, Yujun and Ding, Bolin and Tang, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21016--21029},
  year={2022},
  url={https://proceedings.neurips.cc/paper/2022/hash/a7663702e92787e0e3a4b0e91f1e69d3-Abstract-Conference.html}
}

@inproceedings{huLoRALowRankAdaptation2021,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2106.09685}
}

@article{lialinReLoRAHighRankTraining2023,
  title={ReLoRA: Low-Rank Finetuning Reloaded},
  author={Lialin, Vladimir and Schatz, Arthur},
  journal={arXiv preprint arXiv:2307.09769},
  year={2023},
  url={https://arxiv.org/abs/2307.09769}
}

@article{xiaChainLoRAEfficient2023,
  title={Chain-of-Thought LoRA: Efficiently Steering Large Language Models via Rank-One Model Updates},
  author={Xia, Tianle and Gao, Xin and Yang, Jian and Wang, Xun and Wang, Liwei and Sun, Ming},
  journal={arXiv preprint arXiv:2308.02270},
  year={2023},
  url={https://arxiv.org/abs/2308.02270}
}

@article{renduchintalaTiedLoraEnhacingParameter2023,
  title={Tied LoRA: Enhancing Parameter-Efficient Fine-Tuning with Tied Weights},
  author={Renduchintala, Adithya and Rodriguez, Pedro and Creutz, Mathias},
  journal={arXiv preprint arXiv:2306.13420},
  year={2023},
  url={https://arxiv.org/abs/2306.13420}
}

@article{shengSLoRAServingThousands2023,
  title={S-LoRA: Scalable Efficient Model Serving for Massive LoRA Models},
  author={Sheng, Yi and Han, Xuefei and Zhu, Xuefeng and Yang, Yuanzhi and Sun, Jiani and Zhou, Guohui},
  journal={arXiv preprint arXiv:2306.01125},
  year={2023},
  url={https://arxiv.org/abs/2306.01125}
}

@article{zhangLORAFAMEMORYEFFICIENTLOWRANK2023,
  title={LoRA-FA: Memory-Efficient Low-Rank Adaptation via Feature Re-Alignment},
  author={Zhang, Rui and others},
  journal={arXiv preprint arXiv:2302.05653},
  year={2023},
  url={https://arxiv.org/abs/2302.05653}
}

@article{wangMultiLoRADemocratizingLoRA2023,
  title={Multi-LoRA: Efficient Finetuning for Democratic AI},
  author={Wang, Zihao and Bai, Zhen and Ananiadou, Sophia},
  journal={arXiv preprint arXiv:2305.14377},
  year={2023},
  url={https://arxiv.org/abs/2305.14377}
}

@article{dettmersQLoRAEfficientFinetuning2023,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023},
  url={https://arxiv.org/abs/2305.14314}
}

@article{haoFloraLowRankAdapters2023,
  title={FLORA: Fine-grained Low-Rank Adaptation},
  author={Hao, Yuning and Gu, Shixiang and Liang, Chen},
  journal={arXiv preprint arXiv:2306.17878},
  year={2023},
  url={https://arxiv.org/abs/2306.17878}
}

@article{kamalakaraExploringLowRank2022,
  title={Exploring Low-Rank Training of Deep Neural Networks},
  author={Kamalakara, Himanshu and Kudugunta, Sneha and Sahu, Rohit Prakash and He, He},
  journal={arXiv preprint arXiv:2203.07261},
  year={2022},
  url={https://arxiv.org/abs/2203.07261}
}

@article{wangCuttlefishLowrankModel2023,
  title={Cuttlefish: Low-Rank Model Training Without Factorization},
  author={Wang, Mengzhao and Liu, Zhao and Bai, Yao and Gao, Yuan},
  journal={arXiv preprint arXiv:2305.19635},
  year={2023},
  url={https://arxiv.org/abs/2305.19635}
}

@article{zhaoInRankIncrementalLowRank2023,
  title={In-Rank: Incremental Low-Rank Learning},
  author={Zhao, Yuwei and Zhang, Yifan and others},
  journal={arXiv preprint arXiv:2303.11246},
  year={2023},
  url={https://arxiv.org/abs/2303.11246}
}

@inproceedings{gur-ariGradientDescentHappens2018,
  title={Gradient Descent Happens in a Tiny Subspace},
  author={Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={1778--1787},
  year={2018},
  url={https://proceedings.mlr.press/v80/gur-ari18a.html}
}

@inproceedings{larsenHowManyDegrees2022,
  title={How many degrees of freedom do we need to train deep networks: A loss landscape perspective},
  author={Larsen, Anders Boesen Lindbo and Levina, Elizaveta and Bruna, Joan and S{\o}nderby, Casper Kaae},
  booktitle={International Conference on Machine Learning},
  pages={12273--12285},
  year={2022},
  url={https://proceedings.mlr.press/v162/larsen22a.html}
}

@inproceedings{leeGradientBasedMetaLearningLearned2018,
  title={Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace},
  author={Lee, Keuntaek and Choi, Junsoo and Shin, Jinwoo and Lee, Sung Ju Hwang},
  booktitle={International Conference on Machine Learning},
  pages={2933--2942},
  year={2018},
  url={https://proceedings.mlr.press/v80/lee18a.html}
}

@inproceedings{chaudhryContinualLearningLowrank2020,
  title={Continual Learning with Tiny Memories in Low-Rank Orthogonal Subspaces},
  author={Chaudhry, Arslan and Dokania, Puneet K and Torr, Philip HS},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19566--19578},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/7cce53cf90577442771720a370c3c723-Abstract.html}
}

@inproceedings{chenFastLowrankEstimation2015,
  title={Fast Low-Rank Matrix Estimation without the Condition Number},
  author={Chen, Yu and Wainwright, Martin J},
  booktitle={International Conference on Machine Learning},
  pages={471--479},
  year={2015},
  url={https://proceedings.mlr.press/v37/chenc15.html}
}

@article{chenNonConvexProjectedGradient2019,
  title={Non-convex projected gradient descent for general low-rank matrix recovery},
  author={Chen, Yuxin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={6},
  pages={3541--3558},
  year={2019},
  publisher={IEEE},
  url={https://ieeexplore.ieee.org/document/8584824}
}

@article{zhaoZerOInitializationInitializing2022,
  title={ZerO Initialization: Initializing Neural Networks with Zero-Valued Parameters},
  author={Zhao, Shangqian and Li, Shiyu and Ma, Yi},
  journal={arXiv preprint arXiv:2207.05848},
  year={2022},
  url={https://arxiv.org/abs/2207.05848}
}

@article{cossonLowRankGradientDescent2023,
  title={Low-Rank Gradient Descent Converges and Generalizes},
  author={Cosson, Victor and Lecouat, Baptiste and Varre, Arthur and d'Ascoli, St{\'e}phane and Biroli, Giulio},
  journal={arXiv preprint arXiv:2301.12995},
  year={2023},
  url={https://arxiv.org/abs/2301.12995}
}

@article{yang2023spectral,
  title={Spectral Methods in Low-Rank Model Adaptation},
  author={Yang, Zhilin and Hu, Edward J and Xia, Tianle and Socher, Richard and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.14683},
  year={2023},
  url={https://arxiv.org/abs/2305.14683}
}

@inproceedings{wangATOMOCommunicationefficientLearning2018,
  title={ATOMO: Communication-efficient learning via atomic sparsification},
  author={Wang, Shiqiang and Joshi, Gauri and Ghosh, Sreeram K and Poor, H Vincent},
  booktitle={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9850--9861},
  year={2018},
  url={https://proceedings.neurips.cc/paper/2018/hash/77fd8c838a3a41ee49e699528f2bbaab-Abstract.html}
}

@inproceedings{vogelsPowerGossipPracticalLowRank2020,
  title={PowerGossip: Practical low-rank communication for decentralized optimization},
  author={Vogels, Thijs and Jaggi, Martin and Patrini, Giorgio},
  booktitle={International Conference on Machine Learning},
  pages={10592--10602},
  year={2020},
  url={https://proceedings.mlr.press/v119/vogels20a.html}
}

@article{gooneratneLowrankGradientApproximation2020,
  title={Low-rank gradient approximation for multi-task learning},
  author={Gooneratne, Shamal and Wang, Meng and Guo, Zhili and Kanuparthi, Vamsi Krishna and Rajan, Dinesh and Jayasumana, Anura P},
  journal={arXiv preprint arXiv:2011.01679},
  year={2020},
  url={https://arxiv.org/abs/2011.01679}
}

@article{huangLowRankGradientDescent2023,
  title={Low-Rank Gradient Descent: Fast convergence and low memory cost},
  author={Huang, Zihao and Wu, Lingfei and Xiong, Rui},
  journal={arXiv preprint arXiv:2302.00089},
  year={2023},
  url={https://arxiv.org/abs/2302.00089}
}

@article{modoranuErrorFeedbackCan2023,
  title={Error feedback can make low-precision training more robust},
  author={Modoranu, Teodor and Das, Mrinank and Huang, Po-Sen and Blundell, Charles},
  journal={arXiv preprint arXiv:2302.04970},
  year={2023},
  url={https://arxiv.org/abs/2302.04970}
}

@inproceedings{shazeerAdafactorAdaptiveLearning2018,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  url={https://proceedings.mlr.press/v80/shazeer18a.html}
}

@article{anilMemoryEfficientAdaptive2019,
  title={Memory-efficient adaptive optimization},
  author={Anil, Rohan and Gupta, Vineet and Passos, Alexandre and Shazeer, Noam},
  journal={arXiv preprint arXiv:1901.11150},
  year={2019},
  url={https://arxiv.org/abs/1901.11150}
}

@inproceedings{dettmers8bitOptimizersBlockwise2022,
  title={8-bit optimizers via block-wise quantization},
  author={Dettmers, Tim and Lewis, Mike and Shleifer, Sam and Zettlemoyer, Luke},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://arxiv.org/abs/2110.02861}
}

@article{liMemoryEfficientOptimizers2023,
  title={Memory-Efficient Optimizers for Large-Scale Language Models},
  author={Li, Li and Yang, Shiyu and Chen, Zhe and others},
  journal={arXiv preprint arXiv:2302.05696},
  year={2023},
  url={https://arxiv.org/abs/2302.05696}
}

@article{lvAdaLomoLowmemoryOptimization2023,
  title={AdaLomo: Adaptive Low-Memory Optimization for Large-Scale Deep Learning},
  author={Lv, Shuchang and Zhuang, Rui and Li, Li Erran},
  journal={arXiv preprint arXiv:2301.12712},
  year={2023},
  url={https://arxiv.org/abs/2301.12712}
}

@article{tian2020understanding,
  title={Understanding self-supervised learning dynamics without contrastive pairs},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  journal={arXiv preprint arXiv:2006.08603},
  year={2020},
  url={https://arxiv.org/abs/2006.08603}
}
