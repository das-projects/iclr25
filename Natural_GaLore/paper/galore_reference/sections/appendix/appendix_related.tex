\section{Additional Related Works}

Adafactor \citep{shazeerAdafactorAdaptiveLearning} achieves sub-linear memory cost by factorizing the second-order statistics by a row-column outer product.
\lowrank{} shares similarities with Adafactor in terms of utilizing low-rank factorization to reduce memory cost, but \lowrank{} focuses on the low-rank structure of the gradients, while Adafactor focuses on the low-rank structure of the second-order statistics.

\lowrank{} can reduce the memory cost for both first-order and second-order statistics, and can be combined with Adafactor to achieve further memory reduction. 
In contrast to the previous memory-efficient optimization methods, \lowrank{} operates independently as the optimizers directly receive the low-rank gradients without knowing their full-rank counterparts.

The fused backward operation proposed by LOMO \citep{lvFullParameterFinetuning2023} mitigates the memory cost of storing weight gradients during training.
Integrated with the standard SGD optimizer, LOMO achieves zero optimizer and gradient memory cost during training.
AdaLOMO \citep{lvAdaLomoLowmemoryOptimization2023} enhances this approach by combining the fused backward operation with adaptive learning rate for each parameter, similarly achieving minimal optimizer memory cost.

While LOMO and AdaLOMO represent significant advancements in memory-efficient optimization for fine-tuning or continual pre-training, they might not be directly applicable to pre-training from scratch at larger scales.
For example, the vanilla Adafactor, adopted by AdaLOMO, has been demonstrated to lead to increased training instabilities at larger scales \citep{raeScalingLanguageModels2022,chowdheryPaLMScalingLanguage2022,wortsmanStableLowprecisionTraining,zhaiScalingVisionTransformers2022}.
We believe integrating \lowrank{} with the fused backward operation may offer a promising avenue for achieving memory-efficient large-scale pre-training from scratch.